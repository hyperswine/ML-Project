{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Decision Tree Classification\n",
    "\n",
    "One of the problems with LR were [too many categorical features]. By rethinking the problem, the idea of classification comes to mind.\n",
    "Advantages of DT classifiers are their ability to map complex functions, though with high variance between models.\n",
    "\n",
    "The idea is to then use a simple DT as a baseline then inspect the effect of Random Forests, Bagged & Boosted ensembles of DT's in\n",
    "reducing the variance between models, hopefully ending up with something that performs well on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load up & Clean Data\n",
    "\n",
    "\n",
    "# Some insights about the data\n",
    "\n",
    "\n",
    "# Split data into train, test (1).\n",
    "X_train, X_test, y_train, y_test = []\n",
    "\n",
    "# Split data into '4-folds'\n",
    "fold_1, fold_2, fold_3, fold_4 = []\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Training a DT Classifier\n",
    "\n",
    "Using sklearn, we can learn a simple DT as a baseline of performance between writing our own custom algorithms\n",
    "for higher tunability and streamlining."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "# load up DT classifier\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "# plot the tree (NOTE: might be better to plot into pdf)\n",
    "tree.plot_tree(clf)\n",
    "\n",
    "# test the tree on test data\n",
    "\n",
    "\n",
    "# plot results of test\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Preliminary Analysis\n",
    "\n",
    "[Analysis] = Not bad it seems, works alright but [Flaws] include [Insert Flaws].\n",
    "\n",
    "#### Custom tree algorithms\n",
    "\n",
    "To tune the algorithm a bit more in favor of [Feature], it may be wise to create our own streamlined algorithms\n",
    "that have pre-pruning and flexible max. depths. NOTE: algorithm is based on [CART].\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Class & methods for custom DT\n",
    "\n",
    "class hyper_DT:\n",
    "    \"\"\"\n",
    "    A Decision Tree in a 'hyper' space. Expect a single one of these trees to be quite highly varied\n",
    "    between different samples of the data.\n",
    "    \"\"\"\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train the DT. Note that the data is batched-trained quite fast; the sample size of the data\n",
    "        is not too high & information gain/gini is relatively high for some features.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def info_gain(self):\n",
    "        \"\"\"\n",
    "        :return: The information gain at a certain level.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def gini_index(self):\n",
    "        \"\"\"\n",
    "        :return: The gini index at a certain level. Should be used as main hueristic for splitting nodes.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def predict(self):\n",
    "        \"\"\"\n",
    "\n",
    "        :return: A [0,5] prediction i.e. 'cheap', 'decent', 'mid-range', 'high', 'premium'.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def plot(self):\n",
    "        \"\"\"\n",
    "        Plot the tree. NOTE: tree should be trained first, otherwise a blank page will be plotted.\n",
    "        \"\"\"\n",
    "        pass\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train our custom DT classifier via training data.\n",
    "\n",
    "\n",
    "# Test data. Note that its a pretty [considerable] improvement\n",
    "\n",
    "\n",
    "# Consider 4-fold cross validation results\n",
    "\n",
    "\n",
    "# Plot data\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Results & Analysis\n",
    "\n",
    "Overall, a [considerable improvement]. It is usually better to train our own DT's as we can tune it more thoroughly and coarsely.\n",
    "[More analysis on 4-fold Cross-Validation]\n",
    "[More analysis on plots]\n",
    "\n",
    "#### Ensembling\n",
    "\n",
    "Here, the next step is naturally combining such trees via some of ensembling to try and gain a better performing model.\n",
    "We first consider 'bagging', i.e. bootstrap sampling [75%] of the data at a time and training individual trees.\n",
    "Then when making a prediction, simply average the output.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Bootstrap sampling methods, i.e. sample N examples randomly each time, with pandas rand().\n",
    "\n",
    "\n",
    "# Bagging class & methods\n",
    "class BaggedEnsemble:\n",
    "    \"\"\"\n",
    "    An object to train several of our 'hyper' trees, plot and predict.\n",
    "    \"\"\"\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train multiple trees by bootstrap sampling.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    # TODO more methods to predict (average all the trees), plot (somehow?).\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Training the bagging classifier with training data\n",
    "\n",
    "\n",
    "# Test the classifier with test data\n",
    "\n",
    "\n",
    "# Plot & compare results/differences with sklearn & our single 'hyper' tree\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Effectiveness of the Bagging Ensemble\n",
    "\n",
    "\n",
    "#### Other ensembling methods\n",
    "\n",
    "Finally, we investigate random forests and boosting as alternatives. These methods are expected to outperform\n",
    "anything we have written up to now due to the [nature of the data].\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Random forest class & methods\n",
    "\n",
    "\n",
    "# Boosting classifier class & methods\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}